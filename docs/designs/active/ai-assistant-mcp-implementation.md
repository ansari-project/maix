# AI Assistant with MCP Integration - Implementation Design

## Status: DESIGN PHASE (REVISED)
**Date**: January 10, 2025
**Author**: Claude Code + Human Review Required

---

## 1. DESIGN PHASE - MCP-Integrated Architecture

### 1.1 Overview
Transform the current AI Assistant stub into a fully functional, context-aware assistant powered by Google Gemini that **leverages the existing MCP tools** for all data operations. The AI Assistant becomes a natural language interface to the MCP server, allowing users to interact with Maix through conversation rather than API calls.

### 1.2 Core Architecture Decision

**SELECTED APPROACH: Hybrid MCP + Direct Gemini**

The AI Assistant will:
1. Use **Gemini directly** for natural language understanding and generation
2. Execute actions through **existing MCP tools** (no duplicate API endpoints)
3. Leverage **PAT infrastructure** for secure MCP access
4. Follow the **Event Assistant pattern** already established in `/api/chat/events`

### 1.3 Architecture Flow

```
User Input → AI Assistant UI → /api/ai/chat endpoint
                                        ↓
                              Parse intent with Gemini
                                        ↓
                            Execute via MCP tools (using PAT)
                                        ↓
                              Format response with Gemini
                                        ↓
                                  Return to UI
```

MWK: Streaming is a requirement. 

### 1.4 PAT Management Strategy

Based on the Event Manager implementation:

#### Automatic PAT Generation
```typescript
// Reuse pattern from pat-manager.service.ts
export async function getOrCreateAIAssistantPat(userId: string) {
  // Check for existing AI Assistant PAT
  const existing = await prisma.personalAccessToken.findFirst({
    where: {
      userId,
      name: 'AI Assistant (Auto-generated)',
      revokedAt: null,
      expiresAt: { gt: new Date() }
    }
  })
  
  if (existing) {
    return { token: existing, plainToken: null }
  }
  
  // Create new PAT with 90-day expiry
  const plainToken = generateSecureToken()
  const hashedToken = await hashToken(plainToken)
  
  const token = await prisma.personalAccessToken.create({
    data: {
      userId,
      name: 'AI Assistant (Auto-generated)',
      hashedToken,
      expiresAt: addDays(new Date(), 90),
      metadata: { autoGenerated: true, purpose: 'ai-assistant' }
    }
  })
  
  return { token, plainToken }
}
```

**Key Decisions**:
- ✅ **Silent PAT generation** - User doesn't need to know about PATs
- ✅ **Auto-refresh** - Renew before expiry
- ✅ **Separate from user PATs** - Don't interfere with manual PATs
- ✅ **Encrypted storage** - Follow existing security patterns

### 1.5 MCP Tool Integration

#### Available MCP Tools for AI Assistant

**Content Management**:
- `maix_manage_post` - Create questions, answers, updates
- `maix_search_posts` - Find existing content
- `maix_manage_comment` - Add comments
- `maix_search_comments` - Find discussions

**Project Management**:
- `maix_manage_project` - CRUD operations on projects
- `maix_search_projects` - Find projects
- `maix_manage_todo` - Task management
- `maix_search_todos` - Find tasks

**Product Management**:
- `maix_manage_product` - CRUD on products
- `maix_search_products` - Find products

**Organization Management**:
- `maix_manage_organization` - Org operations
- `maix_manage_organization_member` - Member management

**User Management**:
- `maix_update_profile` - Update user profile
- `maix_manage_personal_project` - Personal projects

MWK: I want ALL the MCP tools available

### 1.6 Gemini Integration with MCP

#### Tool Calling Strategy

**Option 1: Gemini Function Calling → MCP Tools** ❌
- Complex mapping layer
- Double abstraction

MWK: Is there no way we can connect MCP to Gemini directly? Can you check if Anthropic is any better? 

**Option 2: Intent Detection → Direct MCP Calls** ✅ (SELECTED)
```typescript
// Parse user intent with Gemini
const intent = await parseIntent(userMessage)

// Map to MCP tool
switch(intent.action) {
  case 'create_project':
    return await mcpClient.call('maix_manage_project', {
      action: 'create',
      ...intent.params
    })
  case 'find_todos':
    return await mcpClient.call('maix_search_todos', {
      query: intent.query,
      status: intent.filters?.status
    })
}
```

MWK: BAD idea. Dont doo this. 

**Option 3: Hybrid with Tool Descriptions** ✅ (RECOMMENDED)
```typescript
// Provide MCP tool descriptions to Gemini
const systemPrompt = `
You are an AI assistant for Maix. You can perform these actions:

PROJECTS:
- Create project: Use when user wants to start a new project
- Search projects: Use when user asks about existing projects
- Update project: Use when user wants to modify a project

[... other tools ...]

Respond with JSON: {
  "action": "tool_name",
  "parameters": { ... },
  "explanation": "What you're doing"
}
`
```

MWK: Bad idea. 

### 1.7 Conversation Management

#### Following Event Assistant Pattern

```typescript
// Reuse conversation service pattern
interface AIConversation {
  id: string
  userId: string
  messages: ConversationMessage[]
  context: {
    currentPage?: string
    recentActions?: string[]
    activeProject?: string
  }
  createdAt: Date
  updatedAt: Date
}

// Store in database similar to EventConversation
model AIConversation {
  id        String   @id @default(cuid())
  userId    String
  messages  Json     // Stored as JSON
  context   Json?
  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt
  
  user User @relation(fields: [userId], references: [id])
}
```

MWK: We should merge the two designs. Events should adopt the AIConversation stack. 

### 1.8 API Endpoint Design

#### `/api/ai/chat` Route

```typescript
export async function POST(request: NextRequest) {
  // 1. Authenticate user
  const session = await getServerSession(authOptions)
  if (!session?.user?.id) {
    return new Response('Unauthorized', { status: 401 })
  }
  
  // 2. Get or create AI Assistant PAT
  const { token, plainToken } = await getOrCreateAIAssistantPat(session.user.id)
  
  // 3. Initialize MCP client with PAT
  const mcpClient = createMcpClient({
    baseUrl: process.env.NEXT_PUBLIC_URL + '/api/mcp',
    token: plainToken || await decryptToken(token.encryptedToken)
  })
  
  // 4. Parse request
  const { message, conversationId, context } = await request.json()
  
  // 5. Load or create conversation
  const conversation = await getOrCreateConversation(conversationId, session.user.id)
  
  // 6. Process with Gemini
  const genAI = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY })
  
  const enrichedPrompt = buildPromptWithContext({
    message,
    conversation,
    context,
    availableTools: MCP_TOOL_DESCRIPTIONS
  })
  
  const result = await genAI.models.generateContent({
    model: 'gemini-2.0-flash',
    contents: enrichedPrompt,
    generationConfig: {
      temperature: 0.7,
      maxOutputTokens: 2048,
    }
  })

  MWK: Don't forget streaming! 
  
  // 7. Parse AI response and execute MCP tools if needed
  const aiResponse = parseAIResponse(result.text)
  
  if (aiResponse.action) {
    const toolResult = await mcpClient.call(aiResponse.action, aiResponse.parameters)
    aiResponse.result = toolResult
  }
  
  // 8. Update conversation
  await updateConversation(conversation.id, {
    userMessage: message,
    aiResponse
  })
  
  // 9. Return response
  return Response.json({
    message: aiResponse.explanation,
    action: aiResponse.action,
    result: aiResponse.result,
    conversationId: conversation.id
  })
}
```

### 1.9 Security Considerations

#### PAT Security
1. **Automatic PATs are encrypted** - Use existing encryption service
2. **Never expose PATs to client** - Server-side only
3. **Auto-expire after 90 days** - With renewal logic

I don't really care about this. 

4. **Separate from user PATs** - Different naming convention
5. **Audit trail** - Log all MCP operations

#### Prompt Injection Prevention
```typescript
const sanitizeUserInput = (input: string): string => {
  // Remove potential injection attempts
  const cleaned = input
    .replace(/system:/gi, '')
    .replace(/assistant:/gi, '')
    .replace(/\[INST\]/gi, '')
    .slice(0, 1000) // Limit length
  
  return cleaned
}
```

MWK: Overkill for MVP. Remove prompt injection. 

#### Rate Limiting
```typescript
// Per-user rate limits
const rateLimiter = new Map<string, { count: number, resetAt: Date }>()

const checkRateLimit = (userId: string): boolean => {
  const limit = rateLimiter.get(userId)
  const now = new Date()
  
  if (!limit || limit.resetAt < now) {
    rateLimiter.set(userId, {
      count: 1,
      resetAt: new Date(now.getTime() + 60000) // 1 minute window
    })
    return true
  }
  
  if (limit.count >= 20) { // 20 requests per minute
    return false
  }
  
  limit.count++
  return true
}
```

MWK: Overkill for mvp. Remove Rate limiting. 

### 1.10 Context Enrichment Strategy

#### Dynamic Context from MCP

```typescript
const enrichContext = async (mcpClient: McpClient, userId: string) => {
  // Fetch user's recent activity via MCP
  const [recentProjects, activeTodos, recentQuestions] = await Promise.all([
    mcpClient.call('maix_search_projects', {
      authorId: userId,
      limit: 5,
      orderBy: 'updatedAt'
    }),
    mcpClient.call('maix_search_todos', {
      assigneeId: userId,
      status: ['TODO', 'IN_PROGRESS'],
      limit: 10
    }),
    mcpClient.call('maix_search_posts', {
      authorId: userId,
      type: ['QUESTION'],
      limit: 5
    })
  ])
  
  return {
    recentProjects,
    activeTodos,
    recentQuestions,
    userCapabilities: determineUserCapabilities(userId)
  }
}
```

MWK: Not required for MVP. REmove. 

### 1.11 Simplified MVP Scope

#### Phase 1: Read-Only Assistant
- Search and display information
- Answer questions about the platform
- Navigate users to relevant pages
- No data modifications

#### Phase 2: Basic Write Operations
- Create todos
- Ask questions (create posts)
- Update user profile
- Simple project creation

#### Phase 3: Full MCP Integration
- All MCP tools available
- Complex workflows
- Multi-step operations
- Proactive suggestions

MWK: Go straight to Phase 3. 

### 1.12 Testing Strategy

#### Mock MCP Server for Tests
```typescript
// tests/mocks/mcpServer.ts
export const createMockMcpServer = () => {
  return {
    call: jest.fn((tool, params) => {
      switch(tool) {
        case 'maix_search_projects':
          return { projects: mockProjects }
        case 'maix_manage_todo':
          return { success: true, todo: mockTodo }
        default:
          return { success: false, error: 'Unknown tool' }
      }
    })
  }
}
```

### 1.13 Proposed Simplification Options

**IMPORTANT**: These are OPTIONS to prevent over-engineering. Each must be explicitly ACCEPTED or REJECTED during the Align phase.

#### Simplification Option 1: No Conversation Persistence
- **Proposal**: Keep all conversations session-only, no database storage
- **Pros**: 
  - Eliminates privacy concerns
  - No database schema changes needed
  - Reduces storage costs
  - Simpler implementation (no conversation ID management)
- **Cons**: 
  - Users lose context on page refresh
  - Can't resume conversations later
  - No conversation history for debugging
- **Awaiting Decision**: [ ] ACCEPT or [ ] REJECT

REJECT. 

#### Simplification Option 2: Single Model Only (Gemini Flash)
- **Proposal**: Use only gemini-1.5-flash, no model selection
- **Pros**:
  - 10x cheaper than Pro model
  - Faster responses (optimized for speed)
  - Simpler configuration
  - Predictable costs
- **Cons**:
  - Less capable for complex reasoning
  - May struggle with nuanced requests
  - No upgrade path without code changes
- **Awaiting Decision**: [ ] ACCEPT or [ ] REJECT

ACCEPT Single model, but that model should be GEMINI-2.5-Flash. 

#### Simplification Option 3: No Streaming Responses
- **Proposal**: Return complete responses instead of streaming
- **Pros**:
  - Simpler client implementation
  - Easier error handling
  - No WebSocket/SSE complexity
  - Better for caching
- **Cons**:
  - Worse perceived performance
  - Users wait longer for first content
  - Less engaging UX
- **Awaiting Decision**: [ ] ACCEPT or [ ] REJECT

REJECT. Streaming required. 

#### Simplification Option 4: Fixed Context Window (Last 5 Messages)
- **Proposal**: Only send last 5 message pairs to Gemini
- **Pros**:
  - Predictable token usage
  - Lower costs
  - Faster processing
  - Simpler context management
- **Cons**:
  - AI loses earlier context
  - May repeat questions
  - Less coherent long conversations
- **Awaiting Decision**: [ ] ACCEPT or [ ] REJECT

REJECT. 

#### Simplification Option 5: No MCP Tool Explanations
- **Proposal**: AI never mentions which MCP tools it's using
- **Pros**:
  - Cleaner responses
  - Less technical jargon
  - Simpler prompt engineering
  - Users focus on outcomes not process
- **Cons**:
  - Less transparency
  - Harder to debug issues
  - Users can't learn the system
- **Awaiting Decision**: [ ] ACCEPT or [ ] REJECT

REJECT 

#### Simplification Option 6: No Caching Layer
- **Proposal**: Always fetch fresh data from MCP, no caching
- **Pros**:
  - Always up-to-date information
  - No cache invalidation complexity
  - Simpler architecture
  - Less memory usage
- **Cons**:
  - Higher latency
  - More MCP server load
  - Repeated identical queries
- **Awaiting Decision**: [ ] ACCEPT or [ ] REJECT

ACCEPT. 

#### Simplification Option 7: No Rate Limiting Initially
- **Proposal**: Launch without rate limits, add if abused
- **Pros**:
  - Simpler initial implementation
  - Better user experience
  - No false positives blocking legitimate use
- **Cons**:
  - Risk of cost overruns
  - Potential for abuse
  - Harder to add retroactively
- **Awaiting Decision**: [ ] ACCEPT or [ ] REJECT
- 
ACCEPT. 

#### Simplification Option 8: No Custom Error Recovery
- **Proposal**: If MCP fails, show generic error message
- **Pros**:
  - Much simpler error handling
  - Consistent behavior
  - Less code to maintain
- **Cons**:
  - Worse user experience during failures
  - No graceful degradation
  - Users don't know what went wrong
- **Awaiting Decision**: [ ] ACCEPT or [ ] REJECT

REJECT. 

#### Meta-Simplification: Start with ALL Simplifications
- **Proposal**: Accept all simplifications for MVP, add complexity only when proven necessary
- **Pros**:
  - Fastest path to working product
  - Learn from real usage
  - Avoid premature optimization
- **Cons**:
  - May disappoint early users
  - Technical debt if many need reversal
  - Could limit initial adoption
- **Awaiting Decision**: [ ] ACCEPT or [ ] REJECT

REJECT. 

### 1.14 Open Questions for Alignment

#### Tier 1: Critical (Must answer before Plan phase)

1. **Q: Should AI Assistant have full MCP write access immediately?**
   - Option A: Start read-only, gradually add write access ✅ (RECOMMENDED)
   - Option B: Full access from day one (riskier)
   - **Awaiting Decision**

B. 

2. **Q: How should we handle MCP tool failures?**
   - Option A: Fail gracefully with user-friendly message
   - Option B: Retry with exponential backoff
   - Option C: Fallback to direct database queries (breaks abstraction)
   - **Awaiting Decision**

A. 

3. **Q: Should conversations persist across sessions?**
   - Option A: Session-only (simpler, more private) ✅ (RECOMMENDED)
   - Option B: Persist with user consent
   - Option C: Always persist (needs clear privacy policy)
   - **Awaiting Decision**

C. 



4. **Q: Monthly Gemini API budget?**
   - Affects model selection (flash vs pro)
   - Impacts rate limiting strategy
   - **Awaiting Decision**

Don't worry about it. 

#### Tier 2: Important

1. **Q: Should AI explain which MCP tools it's using?**
   - Transparency vs simplicity trade-off

Yes. 

2. **Q: How many conversation turns to keep in context?**
   - Token usage vs context quality

After 20 turns, summarize. 

3. **Q: Should we cache MCP responses?**
   - Performance vs data freshness

No. 

#### Tier 3: Deferrable

1. **Q: Multi-language support?**

No.

2. **Q: Voice input/output?**

No. 

3. **Q: Conversation export?**

No. 

### 1.14 Risk Mitigation

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| MCP server overload | Medium | High | Implement caching, rate limiting |
| PAT expiry during conversation | Low | Medium | Auto-refresh logic, graceful handling |
| Gemini API costs | Medium | High | Use flash model, implement quotas |
| MCP tool changes break AI | Medium | Medium | Version tool descriptions, regression tests |
| Security: Unauthorized MCP access | Low | Critical | Strict PAT validation, audit logging |

MWK: Not worried about these. 

### 1.15 Success Metrics

1. **Functional Metrics**
   - MCP tool success rate > 95%
   - Response time < 3 seconds
   - PAT refresh success rate = 100%

2. **Usage Metrics**
   - Daily active AI users
   - MCP tools used per session
   - Conversation completion rate

3. **Quality Metrics**
   - User satisfaction score
   - Action accuracy (did AI use correct MCP tool?)
   - Context relevance score

MWK: Not worried about these. 

### 1.16 Comparison: Direct API vs MCP Approach

| Aspect | Direct API | MCP Integration |
|--------|------------|-----------------|
| Implementation Speed | Faster | Slower initially |
| Maintenance | Duplicate logic | Single source of truth |
| Security | Custom for each endpoint | Unified PAT system |
| Testing | Mock each endpoint | Mock MCP server |
| Consistency | Risk of divergence | Guaranteed consistency |
| Future Features | Reimplement each | Automatically available |

**Decision**: MCP Integration is the correct approach for long-term maintainability

### 1.17 Expert Review Results

**Date**: January 10, 2025  
**Models Consulted**: Gemini-2.5-Pro (neutral stance), O3 (against stance)

#### Consensus Summary

Both expert models **endorsed the architecture** with high confidence:
- **Gemini-2.5-Pro**: 8/10 confidence - "Technically sound and strategically aligned"
- **O3**: 7.5/10 confidence - "Strong user value proposition with manageable technical risks"

#### Key Points of Agreement

1. **Architecture is Industry Standard**
   - Follows same pattern as GitHub Copilot and Notion AI
   - Server-side LLM integration is proven approach
   - Event-driven pattern provides excellent extensibility

2. **Technical Feasibility**
   - No fundamental blockers identified
   - All components (PAT, MCP, Events) exist in codebase
   - Reusing infrastructure reduces risk

3. **Critical Success Factors**
   - **Cost Management**: Implement budget controls from day one
   - **User Experience**: Success depends on interaction quality
   - **Prompt Engineering**: Requires iterative refinement
   - **Performance**: Keep response under 800ms with streaming

#### Key Expert Recommendations

1. **Cost Controls** (CRITICAL):
   - Implement per-user quotas as MVP feature
   - Average cost: ~$0.015 per session (6k tokens)
   - Caching can reduce costs by 20-30%

2. **Architecture**:
   - Build provider abstraction layer immediately
   - Pre-serialize context to reduce latency
   - Use streaming for better perceived performance

3. **Risks to Mitigate**:
   - Token limits: Gemini ~32k hard cap
   - Event pattern adds ~40ms latency
   - Operational costs are biggest risk

#### Expert Verdict

**PROCEED WITH IMPLEMENTATION** with conditions:
1. Start read-only (Phase 1)
2. Cost controls upfront
3. Provider abstraction from day one
4. Internal beta first
5. Focus on 2-3 killer use cases

---

## 2. ALIGN PHASE - Awaiting Human Decisions

This section will be completed after human review of the Design phase.

### Key Architecture Decisions:

1. **MCP Integration Confirmed**: 
   - [ ] Use MCP tools for all data operations
   - [ ] Reuse PAT infrastructure

2. **PAT Strategy**:
   - [ ] Silent auto-generation for AI Assistant
   - [ ] 90-day expiry with auto-refresh

3. **Conversation Approach**:
   - [ ] Session-only initially
   - [ ] Add persistence later if needed

4. **MVP Scope**:
   - [ ] Phase 1: Read-only operations
   - [ ] Phase 2: Basic writes
   - [ ] Phase 3: Full integration

5. **Critical Questions**:
   - [ ] Write access timeline
   - [ ] Failure handling strategy
   - [ ] Gemini API budget
   - [ ] Privacy/persistence preferences

---

## 3. PLAN PHASE - To Be Completed After Alignment

## 4. PRODUCE PHASE - To Be Completed After Production

## 5. EVALUATE PHASE - To Be Completed After Evaluation

## 6. REFINE PHASE - To Be Completed After Refinement